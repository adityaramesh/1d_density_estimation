{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('/root/code/1d_density_estimation')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from utils.notebook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook explores a few methods for training a flexible 1d density estimator. For the first\n",
    "experiment, we attempt to match a mixture of gaussians using another learned mixture of gaussians.\n",
    "The means and variances of the learned mixture are learned by optimizing the reverse KL divergence\n",
    "directly with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities for mixture distributions.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "def random_gumbel(shape: torch.Size, device: torch.device, eps=1e-12) -> torch.Tensor:\n",
    "    u = torch.empty(shape, dtype=torch.float32, device=device).uniform_(eps, 1 - eps)\n",
    "    return -torch.log(-torch.log(u))\n",
    "\n",
    "def gaussian_mixture_pdf(x: torch.Tensor, pi_logits: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor) -> torch.Tensor:\n",
    "    pi = F.softmax(pi_logits, dim=0)\n",
    "    \n",
    "    x_ = x.unsqueeze(-1)\n",
    "    pi = pi.unsqueeze(0)\n",
    "    mu_ = mu.unsqueeze(0)\n",
    "    sigma_ = sigma.unsqueeze(0)\n",
    "    \n",
    "    f = 1 / (np.sqrt(2 * np.pi) * sigma_) * torch.exp(-1 / 2 * ((x_ - mu_) / sigma_) ** 2)\n",
    "    return torch.sum(pi * f, axis=-1)\n",
    "\n",
    "def relaxed_gaussian_mixture_pdf(x: torch.Tensor, pi_logits: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor, n_samples: int, temp: float = 0.) -> torch.Tensor:\n",
    "    if temp == 0.:\n",
    "        return gaussian_mixture_pdf(x, pi_logits, mu, sigma)\n",
    "    \n",
    "    pi_logits_ = pi_logits.repeat(n_samples).view(n_samples, -1)\n",
    "    pi = F.softmax((pi_logits_ + random_gumbel(pi_logits_.shape, pi_logits_.device)) / temp, dim=-1)\n",
    "    \n",
    "    mu_ = (pi * mu.unsqueeze(0)).sum(dim=1)\n",
    "    sigma_ = torch.sqrt((pi * (sigma ** 2).unsqueeze(0)).sum(dim=1))\n",
    "    \n",
    "    x_ = x.unsqueeze(0)\n",
    "    mu_ = mu_.unsqueeze(-1)\n",
    "    sigma_ = sigma_.unsqueeze(-1)\n",
    "    probs = 1 / (np.sqrt(2 * np.pi) * sigma_) * torch.exp(-1 / 2 * ((x_ - mu_) / sigma_) ** 2)\n",
    "    return probs.mean(dim=0)\n",
    "\n",
    "def gaussian_mixture_log_likelihood(x: torch.Tensor, pi_logits: torch.Tensor, mu: torch.Tensor,\n",
    "    sigma: torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    log_pi = F.log_softmax(pi_logits, dim=0)\n",
    "    \n",
    "    x_ = x.unsqueeze(-1)\n",
    "    log_pi = log_pi.unsqueeze(0)\n",
    "    mu_ = mu.unsqueeze(0)\n",
    "    sigma_ = sigma.unsqueeze(0)\n",
    "    \n",
    "    f = -1 / 2 * ((x_ - mu_) / sigma_) ** 2 - 1 / 2 * np.log(2 * np.pi) - torch.log(sigma_) + log_pi\n",
    "    return torch.logsumexp(f, dim=-1)\n",
    "\n",
    "def relaxed_gaussian_mixture_log_likelihood(x: torch.Tensor, pi_logits: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor, n_samples: int, temp: float = 0.) -> torch.Tensor:\n",
    "    if temp == 0.:\n",
    "        return gaussian_mixture_log_likelihood(x, pi_logits, mu, sigma)\n",
    "    \n",
    "    pi_logits_ = pi_logits.repeat(n_samples).view(n_samples, -1)\n",
    "    pi = F.softmax((pi_logits_ + random_gumbel(pi_logits_.shape, pi_logits_.device)) / temp, dim=-1)\n",
    "    \n",
    "    mu_ = (pi * mu.unsqueeze(0)).sum(dim=1)\n",
    "    sigma_ = torch.sqrt((pi * (sigma ** 2).unsqueeze(0)).sum(dim=1))\n",
    "    \n",
    "    x_ = x.unsqueeze(0)\n",
    "    mu_ = mu_.unsqueeze(-1)\n",
    "    sigma_ = sigma_.unsqueeze(-1)\n",
    "    f = -1 / 2 * ((x_ - mu_) / sigma_) ** 2 - 1 / 2 * np.log(2 * np.pi) - torch.log(sigma_ * n_samples)\n",
    "    return torch.logsumexp(f, dim=0)\n",
    "\n",
    "def gaussian_mixture_sample(count: int, pi_logits: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor,\n",
    "    temp: float = 0.) -> torch.Tensor:\n",
    "    \n",
    "    if temp < 0:\n",
    "        raise ValueError(f\"bad temp {temp}\")\n",
    "    \n",
    "    pi_logits_ = pi_logits.repeat(count).view(count, -1)\n",
    "    \n",
    "    if temp == 0.:\n",
    "        pi = D.Categorical(logits=pi_logits_).sample()\n",
    "        pi = F.one_hot(pi, num_classes=pi_logits.shape[0])\n",
    "    else:\n",
    "        pi = F.softmax((pi_logits_ + random_gumbel(pi_logits_.shape, pi_logits_.device)) / temp, dim=-1)\n",
    "    \n",
    "    mu_ = mu.repeat(count).view(count, -1)\n",
    "    sigma_ = sigma.repeat(count).view(count, -1)\n",
    "    x = D.Normal(loc=mu_, scale=sigma_).sample()\n",
    "    return torch.sum(pi * x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting utilities.\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "import uuid\n",
    "import plotly\n",
    "\n",
    "from PIL             import Image\n",
    "from tqdm            import tqdm\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "def make_layout(width, height):\n",
    "    return go.Layout(width=width, height=height, margin=dict(t=0,r=0,b=0,l=0),\n",
    "        xaxis=dict(gridcolor='rgb(220,220,220)'), yaxis=dict(gridcolor='rgb(220,220,220)'),\n",
    "        plot_bgcolor='rgb(255,255,255)', legend=dict(orientation='h'))\n",
    "\n",
    "def figures_to_gif(figures, duration=200):\n",
    "    images = []\n",
    "    \n",
    "    with tqdm(total=len(figures)) as pbar:\n",
    "        with io.BytesIO() as buf:\n",
    "            plotly.io.write_image(figures[0], format='png', file=buf)\n",
    "            data = buf.getvalue()\n",
    "        with io.BytesIO(data) as buf:\n",
    "            img = Image.open(buf)\n",
    "            img.load()\n",
    "            images.append(img)\n",
    "        pbar.update(1)\n",
    "\n",
    "        for fig in figures[1:]:\n",
    "            with io.BytesIO() as buf:\n",
    "                plotly.io.write_image(fig, format='png', file=buf)\n",
    "                data = buf.getvalue()\n",
    "            with io.BytesIO(data) as buf:\n",
    "                img = Image.open(buf)\n",
    "                img.load()\n",
    "                images.append(img)\n",
    "            pbar.update(1)\n",
    "            \n",
    "    name = str(uuid.uuid4()) + '.gif'\n",
    "    images[0].save(name, save_all=True, append_images=images[1:], optimize=False, duration=duration, loop=0)\n",
    "    return IPImage(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"### Visual confirmation that sampling and PDF evaluation work for normal mixtures:\")\n",
    "\n",
    "mu = torch.Tensor([-12, -4, 4, 12])\n",
    "sigma = torch.Tensor([1, 1, 1, 1])\n",
    "pi_logits = torch.Tensor([0, 1, 2, 3])\n",
    "\n",
    "x = gaussian_mixture_sample(8 * 1024, pi_logits, mu, sigma)\n",
    "t0 = go.Histogram(x=x, histnorm='probability density')\n",
    "\n",
    "x = torch.linspace(-16, 16, 1024)\n",
    "y = gaussian_mixture_pdf(x, pi_logits, mu, sigma)\n",
    "t1 = go.Scatter(x=x, y=y, mode='lines')\n",
    "\n",
    "l = make_layout(width=950, height=600)\n",
    "f = go.Figure(data=[t0, t1], layout=l)\n",
    "py.iplot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"### Visual confirmation that sampling and PDF evaluation work for relaxed mixtures:\")\n",
    "\n",
    "m = torch.linspace(-16, 16, 1024)\n",
    "temps = np.linspace(1, 0, 256)\n",
    "figures = []\n",
    "\n",
    "for t in tqdm(temps):\n",
    "    y = relaxed_gaussian_mixture_pdf(m, pi_logits, mu, sigma, n_samples=1024, temp=t)\n",
    "    t0 = go.Scatter(x=m.numpy(), y=y.numpy(), mode='lines', name=f'pdf (t={t:.2f})', showlegend=True)\n",
    "    \n",
    "    x = gaussian_mixture_sample(8 * 1024, pi_logits, mu, sigma, temp=t)\n",
    "    t1 = go.Histogram(x=x, histnorm='probability density', name=f'hist t={t:.2f}', showlegend=True)\n",
    "    l = make_layout(width=950, height=150)\n",
    "    l.xaxis.update(range=[-16, 16])\n",
    "    l.yaxis.update(range=[0, 0.28])\n",
    "    figures.append(go.Figure(data=[t0, t1], layout=l))\n",
    "    \n",
    "display(figures_to_gif(figures, duration=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"### Bias of Gradient At Optimum, When Fitting a Gaussian Mixture\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sample_size = 1024 * 1024\n",
    "temps = [1 / 1024, 1 / 512, 1 / 256, 1 / 128, 1 / 64, 1 / 32, 1 / 16, 1 / 8, 1 / 4, 1 / 2, 1]\n",
    "\n",
    "pi_logits_grads = []\n",
    "mu_grads = []\n",
    "log_sigma_grads = []\n",
    "\n",
    "pi_logits_grads_relax = []\n",
    "mu_grads_relax = []\n",
    "log_sigma_grads_relax = []\n",
    "\n",
    "for temp in tqdm(temps):\n",
    "    for mode in [0, 1]:\n",
    "        pi_logits_ref = torch.Tensor([0, 1, 2, 3]).cuda()\n",
    "        mu_ref        = torch.Tensor([-12, -4, 4, 12]).cuda()\n",
    "        sigma_ref     = torch.Tensor([1, 1, 1, 1]).cuda()\n",
    "\n",
    "        pi_logits = torch.Tensor([0, 1, 2, 3]).cuda()\n",
    "        mu        = torch.Tensor([-12, -4, 4, 12]).cuda()\n",
    "        log_sigma = torch.zeros_like(sigma_ref).cuda()\n",
    "\n",
    "        pi_logits.requires_grad = True\n",
    "        mu.requires_grad        = True\n",
    "        log_sigma.requires_grad = True\n",
    "\n",
    "        x = gaussian_mixture_sample(sample_size, pi_logits, mu, torch.exp(log_sigma), temp)\n",
    "\n",
    "        if mode == 0:\n",
    "            llq = gaussian_mixture_log_likelihood(x, pi_logits, mu, torch.exp(log_sigma))\n",
    "        elif mode == 1:\n",
    "            llq = relaxed_gaussian_mixture_log_likelihood(x, pi_logits, mu, torch.exp(log_sigma), n_samples=1024, temp=temp)\n",
    "\n",
    "        llp = gaussian_mixture_log_likelihood(x, pi_logits_ref, mu_ref, sigma_ref)\n",
    "        loss = torch.mean(llq - llp)\n",
    "        loss.backward()\n",
    "\n",
    "        if mode == 0:\n",
    "            pi_logits_grads.append(pi_logits.grad.cpu().detach().numpy())\n",
    "            mu_grads.append(mu.grad.cpu().detach().numpy())\n",
    "            log_sigma_grads.append(log_sigma.grad.cpu().detach().numpy())\n",
    "        if mode == 1:\n",
    "            pi_logits_grads_relax.append(pi_logits.grad.cpu().detach().numpy())\n",
    "            mu_grads_relax.append(mu.grad.cpu().detach().numpy())\n",
    "            log_sigma_grads_relax.append(log_sigma.grad.cpu().detach().numpy())\n",
    "    \n",
    "pi_logits_grad_norms = [np.linalg.norm(g) for g in pi_logits_grads]\n",
    "mu_grad_norms        = [np.linalg.norm(g) for g in mu_grads]\n",
    "log_sigma_grad_norms = [np.linalg.norm(g) for g in log_sigma_grads]\n",
    "\n",
    "pi_logits_grad_norms_relax = [np.linalg.norm(g) for g in pi_logits_grads_relax]\n",
    "mu_grad_norms_relax        = [np.linalg.norm(g) for g in mu_grads_relax]\n",
    "log_sigma_grad_norms_relax = [np.linalg.norm(g) for g in log_sigma_grads_relax]\n",
    "\n",
    "x = [np.log(1 / t) / np.log(2) for t in temps]\n",
    "traces = []\n",
    "\n",
    "for name, norms in zip(\n",
    "    ['pi logits', 'mu', 'log sigma', 'pi logits (relax)', 'mu (relax)', 'log sigma (relax)'],\n",
    "    [pi_logits_grad_norms, mu_grad_norms, log_sigma_grad_norms, pi_logits_grad_norms_relax, mu_grad_norms_relax, log_sigma_grad_norms_relax]):\n",
    "    traces.append(go.Scatter(x=x, y=norms, mode='markers+lines', name=name))\n",
    "\n",
    "l = make_layout(width=950, height=600)\n",
    "l.xaxis.update(title='lg(1 / temp)')\n",
    "l.yaxis.update(title='norm of gradient at optimum')\n",
    "f = go.Figure(data=traces, layout=l)\n",
    "py.iplot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show(\"### Fitting a Gaussian Mixture by Optimizing Reverse KL-Divergence (Exact Marginalization)\")\n",
    "\n",
    "show(r\"\"\"Observation: the objective is difficult to optimize using a first-order method. The loss does\n",
    "not always decrease monotonically, and runs are liable to divergence. This happens even with various\n",
    "settings for the step size decay.\"\"\")\n",
    "\n",
    "import torch.optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_batch = 1024\n",
    "\n",
    "# stable up to ~2k iters\n",
    "init_step_size         = 1e-1\n",
    "final_step_size        = init_step_size / 32\n",
    "step_size_anneal_iters = 4 * 1024\n",
    "total_iters            = 32 * 1024\n",
    "\n",
    "def cosine_ramp(start: float, end: float, cur: int, total: int) -> float:\n",
    "    return start + (end - start) * (1 - 1 / 2 * (1 + np.cos(np.pi * min(cur / total, 1))))\n",
    "\n",
    "step_size = lambda t: cosine_ramp(init_step_size, final_step_size, t, step_size_anneal_iters)\n",
    "\n",
    "pi_logits_ref = torch.Tensor([0, 0, 0, 0]).cuda() # TODO: try different logits later\n",
    "mu_ref        = torch.Tensor([-12, -4, 4, 12]).cuda()\n",
    "sigma_ref     = torch.Tensor([1, 1, 1, 1]).cuda()\n",
    "\n",
    "pi_logits = torch.Tensor([0, 0, 0, 0]).cuda()\n",
    "mu        = torch.linspace(-8, 8, 4).cuda() #torch.empty_like(mu_ref).uniform_(-8, 8)\n",
    "#sigma     = torch.ones_like(sigma_ref)\n",
    "log_sigma = torch.zeros_like(sigma_ref).cuda()\n",
    "\n",
    "#pi_logits.requires_grad = True\n",
    "mu.requires_grad = True\n",
    "#log_sigma.requires_grad = True\n",
    "\n",
    "optim = torch.optim.Adam([mu], lr=init_step_size, betas=(0.99, 0.999))\n",
    "losses = []\n",
    "\n",
    "for cur_iter in tqdm(range(total_iters)):\n",
    "    loss = 0\n",
    "    pi = F.softmax(pi_logits, dim=0)\n",
    "\n",
    "    for i in range(pi_logits.shape[0]):\n",
    "        x = D.Normal(loc=mu[i].repeat(n_batch), scale=torch.exp(log_sigma[i]).repeat(n_batch)).sample()\n",
    "        llq = gaussian_mixture_log_likelihood(x, pi_logits, mu, torch.exp(log_sigma))\n",
    "        llp = gaussian_mixture_log_likelihood(x, pi_logits_ref, mu_ref, sigma_ref)\n",
    "        loss += pi[i] * torch.mean(llq - llp)\n",
    "    \n",
    "    losses.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = step_size(cur_iter)\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if cur_iter % 256 == 0 or not torch.isfinite(loss):\n",
    "        print(loss)\n",
    "        print(mu)\n",
    "        \n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"got nonfinite loss; exiting\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = go.Scatter(x=np.arange(len(losses)), y=losses, mode='lines')\n",
    "l = make_layout(width=950, height=600)\n",
    "f = go.Figure(data=[t], layout=l)\n",
    "py.iplot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"### Fitting a Gaussian Mixture by Optimizing Reverse KL-Divergence (Relaxation, Both Quantities, Learned Temp)\")\n",
    "\n",
    "import torch.optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Tuned config for Adam. Different temperature and step size schedules generally did worse.\n",
    "\"\"\"\n",
    "n_batch = 1024\n",
    "\n",
    "init_temp              = 1 / 1\n",
    "final_temp             = 1 / 64\n",
    "temp_anneal_iters      = 2 * 1024\n",
    "total_iters            = 32 * 1024\n",
    "\n",
    "# The smaller the final relaxation temperature, the smaller the final step size to which we\n",
    "# need to decay.\n",
    "init_step_size         = 1e-2\n",
    "final_step_size        = init_step_size / 4\n",
    "step_size_anneal_iters = 2 * 1024\n",
    "\n",
    "def cosine_ramp(start: float, end: float, cur: int, total: int) -> float:\n",
    "    return start + (end - start) * (1 - 1 / 2 * (1 + np.cos(np.pi * min(cur / total, 1))))\n",
    "\n",
    "temp      = 2.3 * torch.ones(1) # Chosen so that inverse_softplus(temp / 4) approx 1.\n",
    "step_size = lambda t: cosine_ramp(init_step_size, final_step_size, t, step_size_anneal_iters)\n",
    "\n",
    "pi_logits_ref = torch.Tensor([0, 0, 0, 0]) # TODO: try different logits later\n",
    "mu_ref        = torch.Tensor([-12, -4, 4, 12])\n",
    "sigma_ref     = torch.Tensor([1, 1, 1, 1])\n",
    "\n",
    "pi_logits = torch.Tensor([0, 0, 0, 0])\n",
    "#mu        = torch.linspace(-8, 8, 4) # This works better, but makes the problem easier.\n",
    "mu        = torch.empty_like(mu_ref).uniform_(-8, 8)\n",
    "sigma     = torch.ones_like(sigma_ref)\n",
    "log_sigma = torch.zeros_like(sigma_ref)\n",
    "\n",
    "temp.requires_grad = True\n",
    "#pi_logits.requires_grad = True\n",
    "mu.requires_grad = True\n",
    "#log_sigma.requires_grad = True # Learning this is still unstable.\n",
    "\n",
    "optim = torch.optim.Adam([temp, mu], lr=init_step_size, betas=(0.99, 0.999))\n",
    "losses = []\n",
    "\n",
    "for cur_iter in tqdm(range(total_iters)):\n",
    "    x = gaussian_mixture_sample(n_batch, pi_logits, mu, torch.exp(log_sigma), F.softplus(temp / 4))\n",
    "    llq = relaxed_gaussian_mixture_log_likelihood(x, pi_logits, mu, torch.exp(log_sigma), n_samples=1024, temp=F.softplus(temp / 4))\n",
    "    llp = gaussian_mixture_log_likelihood(x, pi_logits_ref, mu_ref, sigma_ref)\n",
    "    loss = torch.mean(llq - llp)\n",
    "    losses.append(loss.detach().numpy())\n",
    "    \n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = step_size(cur_iter)\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if cur_iter % 512 == 0 or not torch.isfinite(loss):\n",
    "        print(loss, F.softplus(temp / 4))\n",
    "        print(mu)\n",
    "        \n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"got nonfinite loss; exiting\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = go.Scatter(x=np.arange(len(losses)), y=losses, mode='lines')\n",
    "l = make_layout(width=950, height=600)\n",
    "f = go.Figure(data=[t], layout=l)\n",
    "py.iplot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flow utilities.\n",
    "\"\"\"\n",
    "\n",
    "import attr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing      import Tuple\n",
    "\n",
    "def inverse_softplus(y: float) -> float:\n",
    "    # XXX: this is not a numerically stable implementation.\n",
    "    return np.log(np.exp(y) - 1)\n",
    "\n",
    "@attr.s(eq=False, repr=False)\n",
    "class LocalWarpTransformation(nn.Module):\n",
    "    init_radius:   float        = attr.ib(default=1.)\n",
    "    init_slope:    float        = attr.ib(default=1., validator=lambda i, a, x: x > 0.5)\n",
    "    init_loc:      float        = attr.ib(default=0.)\n",
    "    max_slope:     float        = attr.ib(default=10., validator=lambda i, a, x: x > 0.5)\n",
    "    device:        torch.device = attr.ib(default=None)\n",
    "    requires_grad: bool         = attr.ib(default=True)\n",
    "        \n",
    "    def __attrs_post_init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        r = torch.full((), fill_value=inverse_softplus(self.init_radius), dtype=torch.float32,\n",
    "            device=self.device, requires_grad=self.requires_grad)\n",
    "        s = torch.full((), fill_value=inverse_softplus(self.init_slope - 0.5), dtype=torch.float32,\n",
    "            device=self.device, requires_grad=self.requires_grad)\n",
    "        b = torch.full((), fill_value=self.init_loc, dtype=torch.float32, device=self.device,\n",
    "            requires_grad=self.requires_grad)\n",
    "        \n",
    "        self.r, self.s, self.b = nn.Parameter(r), nn.Parameter(s), nn.Parameter(b)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r, s = F.softplus(self.r), 0.5 + F.softplus(self.s)\n",
    "        return x - r * (2 * F.sigmoid(2 * (1 - s) / r * (x - self.b)) - 1)\n",
    "    \n",
    "    def jacobian(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r, s = F.softplus(self.r), 0.5 + F.softplus(self.s)\n",
    "        z = 2 * (1 - s) / r * (x - self.b)\n",
    "        y = F.sigmoid(z)\n",
    "        return 1 - 4 * (1 - s) * y * (1 - y)\n",
    "    \n",
    "    def inverse(self, y: torch.Tensor, step_size: float = 1., n_iters: int = 16) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inverts the function using Newton's method.\n",
    "        \n",
    "        XXX: this can fail when `self.s` is large; the secant method or bisection are more reliable\n",
    "        choices.\n",
    "        \"\"\"\n",
    "        \n",
    "        x_rec = y\n",
    "        \n",
    "        for _ in range(n_iters):\n",
    "            x_rec = x_rec - step_size * (self.forward(x_rec) - y) / (self.jacobian(x_rec) + 1e-8)\n",
    "            \n",
    "        return x_rec\n",
    "    \n",
    "    def ln_abs_det_jacobian(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # XXX: this computation is likely not stable.\n",
    "        return torch.log(self.jacobian(x))\n",
    "\n",
    "@attr.s(eq=False, repr=False)\n",
    "class AffineTransformation(nn.Module):\n",
    "    device:        torch.device = attr.ib(default=None)\n",
    "    requires_grad: bool         = attr.ib(default=True)\n",
    "        \n",
    "    def __attrs_post_init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        w = torch.ones((), dtype=torch.float32, device=self.device, requires_grad=self.requires_grad)\n",
    "        b = torch.zeros((), dtype=torch.float32, device=self.device, requires_grad=self.requires_grad)\n",
    "        self.w, self.b = nn.Parameter(w), nn.Parameter(b)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w * x + self.b\n",
    "    \n",
    "    def inverse(self, y: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        return (y - self.b) / self.w\n",
    "    \n",
    "    def ln_abs_det_jacobian(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w.abs().log() * torch.ones_like(x)\n",
    "    \n",
    "    def post_update(self) -> None:\n",
    "        pass\n",
    "    \n",
    "@attr.s(eq=False, repr=False)\n",
    "class Flow(nn.Module):\n",
    "    n_blocks:          int          = attr.ib(validator=lambda i, a, x: x >= 1)\n",
    "    init_slope:        float        = attr.ib(default=1.)\n",
    "    device:            torch.device = attr.ib(default=None)\n",
    "    requires_grad:     bool         = attr.ib(default=True)\n",
    "        \n",
    "    def __attrs_post_init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        for k in range(self.n_blocks):\n",
    "            modules.append((f'affine_{k + 1}', AffineTransformation()))\n",
    "            modules.append((f'warp_{k + 1}', LocalWarpTransformation(init_slope=self.init_slope)))\n",
    "\n",
    "        modules.append((f'affine_{self.n_blocks + 1}', AffineTransformation()))\n",
    "        self.modules_ = nn.Sequential(OrderedDict(modules))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.modules_(x)\n",
    "    \n",
    "    def inverse(self, y: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        x = y\n",
    "        \n",
    "        for name, module in reversed(list(self.modules_.named_children())):\n",
    "            x = module.inverse(x, **kwargs)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def forward_with_jacobian_factor(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        y, f = x, torch.zeros_like(x)\n",
    "        \n",
    "        for name, module in self.modules_.named_children():\n",
    "            f += module.ln_abs_det_jacobian(y)\n",
    "            y = module.forward(y)\n",
    "            \n",
    "        return y, f\n",
    "    \n",
    "    def inverse_with_jacobian_factor(self, y: torch.Tensor, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x, f = y, torch.zeros_like(y)\n",
    "        \n",
    "        for name, module in reversed(list(self.modules_.named_children())):\n",
    "            x = module.inverse(x, **kwargs)\n",
    "            f += module.ln_abs_det_jacobian(x)\n",
    "            \n",
    "        return x, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"## Basic Test: Fitting More Gaussian Mixtures\")\n",
    "\n",
    "import torch.optim\n",
    "\n",
    "# Successful configuration for three-mode mixture.\n",
    "\n",
    "n_batch = 32 * 1024\n",
    "\n",
    "pi_logits_ref = torch.Tensor([0, 0, 0])\n",
    "mu_ref        = torch.Tensor([0, 2, 4])\n",
    "sigma_ref     = torch.Tensor([1 / 2, 1 / 2, 1 / 2])\n",
    "\n",
    "base_dist = D.Normal(loc=torch.zeros(n_batch), scale=torch.ones(n_batch))\n",
    "f = Flow(n_blocks=16)\n",
    "\n",
    "init_step_size         = 1e-3\n",
    "final_step_size        = 1e-3 / 1\n",
    "step_size_anneal_iters = 8 * 1024\n",
    "total_iters            = 64 * 1024\n",
    "\n",
    "\"\"\"\n",
    "# Here we check that the 16-layer flow with affine layers can still fit a 2d mixture.\n",
    "\n",
    "n_batch = 32 * 1024\n",
    "\n",
    "pi_logits_ref = torch.Tensor([0, 0])\n",
    "mu_ref        = torch.Tensor([-2, 2])\n",
    "sigma_ref     = torch.Tensor([1 / 2, 1 / 2])\n",
    "\n",
    "base_dist = D.Normal(loc=torch.zeros(n_batch), scale=torch.ones(n_batch))\n",
    "f = Flow(use_affine_layers=True, n_blocks=16)\n",
    "\n",
    "init_step_size         = 1e-3\n",
    "final_step_size        = 1e-3 / 1\n",
    "step_size_anneal_iters = 8 * 1024\n",
    "total_iters            = 64 * 1024\n",
    "\"\"\"\n",
    "\n",
    "def cosine_ramp(start: float, end: float, cur: int, total: int) -> float:\n",
    "    return start + (end - start) * (1 - 1 / 2 * (1 + np.cos(np.pi * min(cur / total, 1))))\n",
    "\n",
    "step_size = lambda t: cosine_ramp(init_step_size, final_step_size, t, step_size_anneal_iters)\n",
    "optim = torch.optim.Adam(list(f.parameters()), lr=init_step_size, betas=(0.99, 0.999))\n",
    "\n",
    "losses = []\n",
    "params = {k : [] for k, _ in f.named_parameters()}\n",
    "\n",
    "for cur_iter in range(total_iters):\n",
    "    x = base_dist.sample()\n",
    "    y, d = f.forward_with_jacobian_factor(x)\n",
    "    llq = (-1 / 2 * np.log(2 * np.pi) - x ** 2 / 2) - d\n",
    "    llp = gaussian_mixture_log_likelihood(y, pi_logits_ref, mu_ref, sigma_ref)\n",
    "    loss = (llq - llp).mean()\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    has_bad_grad = False\n",
    "    \n",
    "    for name, param in f.named_parameters():\n",
    "        if not torch.isfinite(param.grad):\n",
    "            print(f\"grad for param {name} has nonfinite values (step {cur_iter})\")\n",
    "            has_bad_grad = True\n",
    "            \n",
    "    if has_bad_grad:\n",
    "        break\n",
    "    \n",
    "    optim.step()\n",
    "    losses.append(float(loss.detach().numpy()))\n",
    "    \n",
    "    for k, v in f.named_parameters():\n",
    "        params[k].append(float(v.detach().numpy()))\n",
    "    \n",
    "    if cur_iter % 512 == 0 or not torch.isfinite(loss):\n",
    "        print(cur_iter, loss)\n",
    "        \n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"got nonfinite loss; exiting\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"### Visualization of Transformed Base Density\")\n",
    "\n",
    "import io\n",
    "import plotly\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "x = D.Normal(loc=torch.zeros(8 * 1024), scale=torch.ones(8 * 1024)).sample()\n",
    "y = x\n",
    "\n",
    "t = go.Histogram(x=y.numpy(), histnorm='probability density')\n",
    "l = make_layout(width=950, height=150)\n",
    "l.xaxis.update(range=[-3, 6])\n",
    "l.yaxis.update(range=[0, 0.6])\n",
    "\n",
    "figures = [go.Figure(data=[t], layout=l)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, module in f.modules_.named_children():\n",
    "        y = module.forward(y)\n",
    "        t = go.Histogram(x=y.numpy(), histnorm='probability density')\n",
    "        figures.append(go.Figure(data=[t], layout=l))\n",
    "            \n",
    "display(figures_to_gif(figures, duration=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"## Basic Test: Fitting Laplace Distribution\")\n",
    "\n",
    "import torch.optim\n",
    "\n",
    "def laplace_log_likelihood(x: torch.Tensor, mu: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    return -torch.log(2 * b) - torch.abs(x - mu) / b\n",
    "\n",
    "\"\"\"\n",
    "# Working configuration for learning Laplace(0, 1). The model was unable to match the target density\n",
    "# well unless the slope parameters were initialized to values less than 1. Here, we use 0.6.\n",
    "\n",
    "mu_ref = torch.zeros(1)\n",
    "b_ref  = torch.ones(1)\n",
    "\n",
    "n_batch = 32 * 1024\n",
    "\n",
    "base_dist = D.Normal(loc=torch.zeros(n_batch), scale=torch.ones(n_batch))\n",
    "f = Flow(n_blocks=16, init_slope=0.6)\n",
    "\n",
    "init_step_size         = 1e-3\n",
    "final_step_size        = 1e-3 / 1\n",
    "step_size_anneal_iters = 8 * 1024\n",
    "total_iters            = 64 * 1024\n",
    "\"\"\"\n",
    "\n",
    "mu_ref = torch.zeros(1)\n",
    "b_ref  = torch.full((1,), fill_value=1 / 16)\n",
    "\n",
    "n_batch = 32 * 1024\n",
    "\n",
    "base_dist = D.Normal(loc=torch.zeros(n_batch), scale=torch.ones(n_batch))\n",
    "f = Flow(n_blocks=16, init_slope=0.6)\n",
    "\n",
    "init_step_size         = 1e-3\n",
    "final_step_size        = 1e-3 / 1\n",
    "step_size_anneal_iters = 8 * 1024\n",
    "total_iters            = 64 * 1024\n",
    "\n",
    "def cosine_ramp(start: float, end: float, cur: int, total: int) -> float:\n",
    "    return start + (end - start) * (1 - 1 / 2 * (1 + np.cos(np.pi * min(cur / total, 1))))\n",
    "\n",
    "step_size = lambda t: cosine_ramp(init_step_size, final_step_size, t, step_size_anneal_iters)\n",
    "optim = torch.optim.Adam(list(f.parameters()), lr=init_step_size, betas=(0.99, 0.999))\n",
    "\n",
    "losses = []\n",
    "params = {k : [] for k, _ in f.named_parameters()}\n",
    "\n",
    "for cur_iter in range(total_iters):\n",
    "    x = base_dist.sample()\n",
    "    y, d = f.forward_with_jacobian_factor(x)\n",
    "    llq = (-1 / 2 * np.log(2 * np.pi) - x ** 2 / 2) - d\n",
    "    llp = laplace_log_likelihood(y, mu_ref, b_ref)\n",
    "    loss = (llq - llp).mean()\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    has_bad_grad = False\n",
    "    \n",
    "    for name, param in f.named_parameters():\n",
    "        if not torch.isfinite(param.grad):\n",
    "            print(f\"grad for param {name} has nonfinite values (step {cur_iter})\")\n",
    "            has_bad_grad = True\n",
    "            \n",
    "    if has_bad_grad:\n",
    "        break\n",
    "    \n",
    "    optim.step()\n",
    "    losses.append(float(loss.detach().numpy()))\n",
    "    \n",
    "    for k, v in f.named_parameters():\n",
    "        params[k].append(float(v.detach().numpy()))\n",
    "    \n",
    "    if cur_iter % 512 == 0 or not torch.isfinite(loss):\n",
    "        print(cur_iter, loss)\n",
    "        \n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"got nonfinite loss; exiting\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"### Visualization of Transformed Base Density\")\n",
    "\n",
    "import io\n",
    "import plotly\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "x = D.Normal(loc=torch.zeros(8 * 1024), scale=torch.ones(8 * 1024)).sample()\n",
    "y = x\n",
    "\n",
    "t = go.Histogram(x=y.numpy(), histnorm='probability density')\n",
    "l = make_layout(width=950, height=150)\n",
    "l.xaxis.update(range=[-3, 6])\n",
    "l.yaxis.update(range=[0, 0.6])\n",
    "\n",
    "figures = [go.Figure(data=[t], layout=l)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, module in f.modules_.named_children():\n",
    "        y = module.forward(y)\n",
    "        t = go.Histogram(x=y.numpy(), histnorm='probability density')\n",
    "        figures.append(go.Figure(data=[t], layout=l))\n",
    "            \n",
    "display(figures_to_gif(figures, duration=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
